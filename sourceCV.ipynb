{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[K     |███████████████████▋            | 314.2 MB 105.6 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 511.7 MB 43 kB/s s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (1.22.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 26.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (3.19.4)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 21.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (1.44.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 33.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 32.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./anaconda3/lib/python3.8/site-packages (from tensorflow) (20.4)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 5.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in ./anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./anaconda3/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.8/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in ./anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in ./anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./anaconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in ./anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.4.0)\n",
      "Installing collected packages: keras, tensorboard, google-pasta, astunparse, flatbuffers, keras-preprocessing, tensorflow-io-gcs-filesystem, libclang, gast, tensorflow-estimator, opt-einsum, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.8.0\n",
      "    Uninstalling tensorboard-2.8.0:\n",
      "      Successfully uninstalled tensorboard-2.8.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-pasta-0.2.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 opt-einsum-3.3.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.8/site-packages (1.22.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(dataFile,maxSequenceLength,embeddings):\n",
    "\tlabels=[]\n",
    "\tsources=[]\n",
    "\twordsD={'<PAD>':0,'<UNK>':1}\n",
    "\twordsL=['<PAD>','<UNK>']\n",
    "\tcontents=[]\n",
    "\tnums=[]\n",
    "\tdocumentCVs=[]\n",
    "\ttopicCVs=[]\n",
    "\tsourceCVs=[]\n",
    "\tfileIn=open(dataFile,'r')\n",
    "\twhile True:\n",
    "\t\tline=fileIn.readline()\n",
    "\t\tif line=='':\n",
    "\t\t\tbreak\n",
    "\t\tparts=line.strip().split('\\t')\n",
    "\t\tlabel=parts[0]\n",
    "\t\tsource=parts[1]\n",
    "\t\tnum=parts[2]\n",
    "\t\tdocumentCV=parts[3]\n",
    "\t\ttopicCV=parts[4]\n",
    "\t\tsourceCV=parts[5]\n",
    "\t\tcontentWords=parts[6:]\n",
    "\t\tif len(contentWords)>maxSequenceLength:\n",
    "\t\t\tcontentWords=contentWords[0:maxSequenceLength]\n",
    "\t\tcontent=[]\n",
    "\t\tfor contentWord in contentWords:\n",
    "\t\t\tif not contentWord in embeddings:\n",
    "\t\t\t\tcontentWord='<UNK>'\n",
    "\t\t\telif not contentWord in wordsD:\n",
    "\t\t\t\twordsD[contentWord]=len(wordsD)\n",
    "\t\t\t\twordsL.append(contentWord)\n",
    "\t\t\tcontent.append(wordsD[contentWord])\n",
    "\t\tif len(content)<maxSequenceLength:\n",
    "\t\t\tcontent=content+[0]*(maxSequenceLength-len(content))\t\n",
    "\t\tlabels.append(label)\n",
    "\t\tsources.append(source)\n",
    "\t\tcontents.append(content)\n",
    "\t\tnums.append(num)\n",
    "\t\tdocumentCVs.append(documentCV)\n",
    "\t\ttopicCVs.append(topicCV)\n",
    "\t\tsourceCVs.append(sourceCV)\n",
    "\tfileIn.close()\n",
    "\treturn(labels,sources,nums,wordsL,contents,documentCVs,topicCVs,sourceCVs)\n",
    "\n",
    "\n",
    "def readDocuments(dataFile,maxSequenceLength,embeddings,maxDocumentLength):\n",
    "\t(labels,sources,nums,wordsL,contents,documentCVs,topicCVs,sourceCVs)=readData(dataFile,maxSequenceLength,embeddings)\n",
    "\tlabelsD=[]\n",
    "\tsourcesD=[]\n",
    "\tnumsD=[]\n",
    "\tcontentsD=[]\n",
    "\tdocumentCVsD=[]\n",
    "\ttopicCVsD=[]\n",
    "\tsourceCVsD=[]\n",
    "\tlengthsD=[]\n",
    "\ti=0\n",
    "\tcontentHere=[]\n",
    "\twhile(True):\n",
    "\t\tcontentHere.append(contents[i])\n",
    "\t\tif i==len(nums)-1 or nums[i+1]!=nums[i]:\n",
    "\t\t\tlabelsD.append(labels[i])\n",
    "\t\t\tsourcesD.append(sources[i])\n",
    "\t\t\tnumsD.append(nums[i])\n",
    "\t\t\tdocumentCVsD.append(documentCVs[i])\n",
    "\t\t\ttopicCVsD.append(topicCVs[i])\n",
    "\t\t\tsourceCVsD.append(sourceCVs[i])\n",
    "\t\t\tif len(contentHere)<maxDocumentLength:\n",
    "\t\t\t\tlengthHere=len(contentHere)\n",
    "\t\t\t\tfor j in range(maxDocumentLength-len(contentHere)):\n",
    "\t\t\t\t\tcontentHere.append([0]*maxSequenceLength)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlengthHere=maxDocumentLength\n",
    "\t\t\t\tcontentHere=contentHere[0:maxDocumentLength]\n",
    "\t\t\tlengthsD.append(lengthHere)\n",
    "\t\t\tcontentsD.append(contentHere)\n",
    "\t\t\tcontentHere=[]\n",
    "\t\ti=i+1\n",
    "\t\tif i==len(nums):\n",
    "\t\t\tbreak\n",
    "\treturn(labelsD,sourcesD,numsD,wordsL,contentsD,documentCVsD,topicCVsD,sourceCVsD,lengthsD)\n",
    "\n",
    "\n",
    "def readEmbeddings(vectorsFile):\n",
    "\tembeddings={}\n",
    "\tfor line in open(vectorsFile):\n",
    "\t\tparts=line.split()\n",
    "\t\tif len(parts)!=301:\n",
    "\t\t\tcontinue\n",
    "\t\tcoefs=np.asarray(parts[1:],dtype='float32')\n",
    "\t\tembeddings[parts[0]]=coefs\n",
    "\treturn(embeddings)\n",
    "\n",
    "def prepareEmbeddings(embeddings,wordsL):\n",
    "\tembeddingMatrix = np.zeros((len(wordsL), 300))\n",
    "\tfor i in range(len(wordsL)):\n",
    "\t\trow=np.zeros(300)\n",
    "\t\tword=wordsL[i]\n",
    "\t\tif word!='<PAD>' and word!='<UNK>':\n",
    "\t\t\trow=embeddings[word]\n",
    "\t\t\trow=row/np.sqrt(sum(row*row))\n",
    "\t\tembeddingMatrix[i]=row\n",
    "\treturn(embeddingMatrix)\n",
    "\n",
    "def lengthToAverageMask(lengthD,maxDocumentLength,binary=False):\n",
    "\tresult=[]\n",
    "\tfor i in range(len(lengthD)):\n",
    "\t\tif binary:\n",
    "\t\t\tmultiplier=1.0\n",
    "\t\telse:\n",
    "\t\t\tmultiplier=maxDocumentLength*1.0/lengthD[i]\n",
    "\t\tvec=[multiplier,multiplier]\n",
    "\t\tvec0=[0.0,0.0]\n",
    "\t\tvector=[vec]*lengthD[i]+[vec0]*(maxDocumentLength-lengthD[i])\n",
    "\t\tresult.append(vector)\n",
    "\treturn(np.array(result))\n",
    "\n",
    "\t\n",
    "def evaluateD(resultPred,resultTrue):\n",
    "\treturn(np.mean((resultPred[:,1]>0.5)==(resultTrue[:,1]==1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Style1():\n",
    "\tdef __init__(self,embeddingMatrix,labelsNum,maxSequenceLength,maxDocumentLength,onGPU):\n",
    "\t\tself.maxSequenceLength=maxSequenceLength\n",
    "\t\tself.maxDocumentLength=maxDocumentLength\n",
    "\t\tself.labelsNum=labelsNum\n",
    "\t\t\n",
    "\t\tself.embeddingSize=np.shape(embeddingMatrix)[1]\n",
    "\t\tself.embL=keras.layers.Embedding(np.shape(embeddingMatrix)[0],self.embeddingSize,input_length=maxSequenceLength,weights=[embeddingMatrix],trainable=False)\n",
    "\t\tself.reshape1L=keras.layers.Lambda(self.backend_reshape,output_shape=(maxSequenceLength,self.embeddingSize))\n",
    "\t\tif onGPU:\n",
    "\t\t\tself.LSTMforL=keras.layers.LSTM(units=100,go_backwards=False,return_sequences=False)\n",
    "\t\t\tself.LSTMrevL=keras.layers.LSTM(units=100,go_backwards=True,return_sequences=False)\n",
    "\t\telse:\n",
    "\t\t\tself.LSTMforL=keras.layers.LSTM(units=100,go_backwards=False,return_sequences=False)\n",
    "\t\t\tself.LSTMrevL=keras.layers.LSTM(units=100,go_backwards=True,return_sequences=False)\n",
    "\t\tself.conL=keras.layers.Concatenate(axis=1)\n",
    "\t\tself.denseL=keras.layers.Dense(labelsNum,activation=\"softmax\")\n",
    "\t\tself.reshape2L=keras.layers.Lambda(self.backend_reshape2,output_shape=(maxDocumentLength,labelsNum))\n",
    "\t\tself.multiply=keras.layers.Multiply()\n",
    "\t\tself.poolingL=keras.layers.GlobalAveragePooling1D()\n",
    "\n",
    "\tdef backend_reshape(self,x):\n",
    "\t\treturn keras.backend.reshape(x,(-1,self.maxSequenceLength,self.embeddingSize))\n",
    "\n",
    "\tdef backend_reshape2(self,x):\n",
    "\t\treturn keras.backend.reshape(x,(-1,self.maxDocumentLength,self.labelsNum))\n",
    "\n",
    "\tdef getMask(self,lengthD):\n",
    "\t\treturn (lengthToAverageMask(lengthD,self.maxDocumentLength,binary=False))\n",
    "\n",
    "\tdef getModel(self):\n",
    "\t\tinputWords = keras.layers.Input(shape=(self.maxDocumentLength,self.maxSequenceLength,))\n",
    "\t\tinputMask = keras.layers.Input(shape=(self.maxDocumentLength,self.labelsNum,))\n",
    "\t\tx=self.embL(inputWords)\n",
    "\t\tx=self.reshape1L(x)\n",
    "\t\tlstm1=self.LSTMforL(x)\n",
    "\t\tlstm2=self.LSTMrevL(x)\n",
    "\t\tx=self.conL([lstm1,lstm2])\n",
    "\t\tPs=self.denseL(x)\n",
    "\t\tPs=self.reshape2L(Ps)\n",
    "\t\tPs=self.multiply([Ps,inputMask])\n",
    "\t\tPs=self.poolingL(Ps)\n",
    "\t\tmodel=Model(inputs=[inputWords,inputMask], outputs=Ps)\n",
    "\t\treturn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Converting to numpy\n"
     ]
    }
   ],
   "source": [
    "# Local setup\n",
    "dataPath=\"./\"\n",
    "batch_size=64\n",
    "onGPU=True\t\n",
    "epochs=10\n",
    "dataset=\"all_data.tsv\"\n",
    "\n",
    "print('Reading data')\n",
    "# Reading data\n",
    "\n",
    "embeddings=readEmbeddings(dataPath+\"GoogleNewsUnigrams.txt\")\n",
    "MAX_SEQUENCE_LENGTH=120\n",
    "MAX_DOCUMENT_LENGTH=50\n",
    "(labels,sources,nums,wordsL,contents,documentCV,topicCV,sourceCV,lengthD)=readDocuments(dataPath+dataset,MAX_SEQUENCE_LENGTH,embeddings,MAX_DOCUMENT_LENGTH)\n",
    "embeddingMatrix=prepareEmbeddings(embeddings,wordsL)\n",
    "\n",
    "print(\"Converting to numpy\")\n",
    "# Converting to numpy\n",
    "y=np.asarray(labels,dtype='float32')\n",
    "allY=np.concatenate((np.expand_dims(1-y,1),np.expand_dims(y,1)),axis=1)\n",
    "allX=np.array(contents)\n",
    "\n",
    "\n",
    "# Prepare CV scenarios\n",
    "documentCV=np.asarray(documentCV,dtype='int32')\n",
    "topicCV=np.asarray(topicCV,dtype='int32')\n",
    "sourceCV=np.asarray(sourceCV,dtype='int32')\n",
    "scenarioCV=sourceCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Evaluating on fold 1...\n",
      "Epoch 1/10\n",
      "537/537 [==============================] - 3865s 7s/step - loss: 0.4113 - accuracy: 0.8277 - val_loss: 0.6190 - val_accuracy: 0.6566\n",
      "Epoch 2/10\n",
      "537/537 [==============================] - 3896s 7s/step - loss: 0.3380 - accuracy: 0.8567 - val_loss: 0.6371 - val_accuracy: 0.6566\n",
      "Epoch 3/10\n",
      "537/537 [==============================] - 3896s 7s/step - loss: 0.3106 - accuracy: 0.8724 - val_loss: 0.7861 - val_accuracy: 0.5838\n",
      "Epoch 4/10\n",
      "537/537 [==============================] - 3903s 7s/step - loss: 0.2982 - accuracy: 0.8778 - val_loss: 0.5622 - val_accuracy: 0.7075\n",
      "Epoch 5/10\n",
      "537/537 [==============================] - 3901s 7s/step - loss: 0.2875 - accuracy: 0.8816 - val_loss: 0.8417 - val_accuracy: 0.5822\n",
      "Epoch 6/10\n",
      "537/537 [==============================] - 3899s 7s/step - loss: 0.2805 - accuracy: 0.8853 - val_loss: 0.6009 - val_accuracy: 0.6745\n",
      "Epoch 7/10\n",
      "537/537 [==============================] - 3903s 7s/step - loss: 0.2738 - accuracy: 0.8881 - val_loss: 0.6499 - val_accuracy: 0.6568\n",
      "Epoch 8/10\n",
      "537/537 [==============================] - 3904s 7s/step - loss: 0.2834 - accuracy: 0.8844 - val_loss: 0.5870 - val_accuracy: 0.6819\n",
      "Epoch 9/10\n",
      "537/537 [==============================] - 3907s 7s/step - loss: 0.2674 - accuracy: 0.8919 - val_loss: 0.6526 - val_accuracy: 0.6504\n",
      "Epoch 10/10\n",
      "537/537 [==============================] - 3908s 7s/step - loss: 0.2597 - accuracy: 0.8970 - val_loss: 0.6553 - val_accuracy: 0.6548\n",
      "315/720 [============>.................] - ETA: 8:50"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Start')\n",
    "result=np.array([[-1,-1]]*len(scenarioCV),dtype='float32')\n",
    "for folda in range(max(scenarioCV)):\n",
    "\tfold=folda+1\n",
    "\tprint(\"Evaluating on fold \"+str(fold)+\"...\")\n",
    "\twhichTest=np.isin(scenarioCV,fold)\n",
    "\ttrainY=allY[~whichTest,]\n",
    "\tdevelY=allY[whichTest,]\n",
    "\ttrainX=allX[~whichTest,]\n",
    "\tdevelX=allX[whichTest,]\n",
    "\tstyle1=Style1(embeddingMatrix,2,MAX_SEQUENCE_LENGTH,MAX_DOCUMENT_LENGTH,onGPU)\n",
    "\tmask=style1.getMask(lengthD)\n",
    "\ttrainM=np.array(mask)[~whichTest]\n",
    "\tdevelM=np.array(mask)[whichTest]\n",
    "\tmodel=style1.getModel()\n",
    "\tmodel.compile(optimizer=tf.optimizers.Adam(),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\tfit=model.fit([trainX,trainM],trainY, epochs=epochs,validation_data=([develX,develM],develY),batch_size=batch_size)\n",
    "\tpredictions=model.predict([develX,develM])\n",
    "\tresult[whichTest,:]=predictions\n",
    "\n",
    "evaluateD(result,allY)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
